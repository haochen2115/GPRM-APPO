import argparse
import re
from typing import List, Optional
import uvicorn
import torch
from fastapi import FastAPI, Request
from fastapi.responses import JSONResponse
from transformers import AutoTokenizer, AutoModelForCausalLM
from openrlhf.utils.logging_utils import init_logger

logger = init_logger(__name__)

class GenPRM:
    def __init__(self, args) -> None:
        # åˆå§‹åŒ–tokenizer
        self.tokenizer = AutoTokenizer.from_pretrained(
            args.reward_pretrain,
            trust_remote_code=True,
            use_fast=False
        )

        # åˆå§‹åŒ–æ¨¡å‹
        self.model = AutoModelForCausalLM.from_pretrained(
            args.reward_pretrain,
            torch_dtype="auto",
            device_map="auto",
            trust_remote_code=True
        )
        
        self.max_length = args.max_len
        self.placeholder_token = args.placeholder_token
        
    def prepare_prompt(self, text: str) -> str:
        return f"""<|im_start|>user
è¯·ä½ æ ¹æ®å½“å‰çš„é—®é¢˜å’Œæ¨¡å‹è¾“å‡ºçš„ä¸­é—´æ­¥éª¤è¿›è¡Œå¾—åˆ†ï¼Œæ³¨æ„åªéœ€è¦å¯¹æœ€åä¸€ä¸ªæ­¥éª¤è¿›è¡Œè¯„ä»·å³å¯ã€‚ä½ éœ€è¦å…ˆè¾“å‡ºåˆ¤æ–­çš„è¿‡ç¨‹ï¼Œå†è¾“å‡ºåˆ¤æ–­çš„3ä¸ªæ ‡è®°[ <|+|> , <|-|> , <|o|> ]
{text}
è¯·è¾“å‡ºä½ çš„ç‚¹è¯„å’Œæ ‡ç­¾ï¼š
<|im_end|>
<|im_start|>assistant
"""

    def process_response(self, response: str) -> float:
        """å¤„ç†ç”Ÿæˆçš„å“åº”,æå–åˆ†æ•°"""
        gen_part = response.split("<|im_start|>assistant")[-1]
        
        if "<|+|>" in gen_part:
            print(" âœ… ",gen_part)
            score = 1.0
        elif "<|-|>" in gen_part:
            print(" âŒ ",gen_part)
            score = -1.0
        else:
            print(" ğŸ€„ï¸ ",gen_part)
            score = 0.0
            
        return score

    def get_reward(self, queries: List[str]) -> List[List[float]]:
        """æ‰¹é‡è·å–å¥–åŠ±åˆ†æ•°,è¿”å›tokençº§åˆ«çš„reward"""
        all_rewards = []
        
        # å°†queriesè½¬æ¢ä¸ºtoken ids
        tokenized = self.tokenizer(
            queries,
            return_tensors="pt",
            add_special_tokens=False,
            max_length=self.max_length,
            padding=True,
            truncation=True,
        )
        input_ids = tokenized["input_ids"].to(self.model.device)
        attention_mask = tokenized["attention_mask"].to(self.model.device)
        
        for i, (ids, mask) in enumerate(zip(input_ids, attention_mask)):
            # åˆå§‹åŒ–tokençº§åˆ«çš„reward
            reward = torch.zeros_like(ids, dtype=torch.float)
            
            # æ‰¾åˆ°æ‰€æœ‰placeholder_tokençš„ä½ç½®
            placeholder_id = self.tokenizer.convert_tokens_to_ids(self.placeholder_token)
            step_indices = (ids == placeholder_id).nonzero(as_tuple=True)[0]
            
            if len(step_indices) < 5 or len(step_indices) > 50:
                logger.warning(f"""ğŸ›[gen prm]
                ğŸš¨ Steps count {len(step_indices)} is abnormal for query {i} :
                {queries[i]}
                ğŸš¨ 
                """)
            
            # å¯¹æ¯ä¸ªstepä½ç½®ç”Ÿæˆè¯„åˆ†
            for step_idx in step_indices:
                # æ„å»ºåˆ°å½“å‰stepçš„prompt
                prefix = ids[:step_idx+1].tolist()
                curr_input = self.prepare_prompt(self.tokenizer.decode(prefix))
                
                # è½¬æ¢è¾“å…¥
                inputs = self.tokenizer(curr_input, return_tensors="pt").to(self.model.device)
                
                # ç”Ÿæˆè¯„åˆ†
                with torch.no_grad():
                    outputs = self.model.generate(
                        **inputs,
                        max_new_tokens=512,
                        temperature=0.01,
                        top_p=0.9,
                        pad_token_id=self.tokenizer.pad_token_id,
                        eos_token_id=self.tokenizer.eos_token_id,
                    )
                generated_text = self.tokenizer.decode(outputs[0], skip_special_tokens=False)
                score = self.process_response(generated_text)
                
                # åœ¨å¯¹åº”çš„tokenä½ç½®è®¾ç½®reward
                reward[step_idx] = score
                
            all_rewards.append(reward.tolist())
            logger.info(f"Query {i}: {len(step_indices)} steps, sum reward: {torch.sum(reward)}")
        
        logger.info(f"all_rewards[0]: {all_rewards[0]}, len(all_rewards[0]): {len(all_rewards[0])}")

        return all_rewards

if __name__ == "__main__":
    parser = argparse.ArgumentParser()
    # Model params
    parser.add_argument("--reward_pretrain", type=str, required=True)
    parser.add_argument("--max_len", type=int, default=2048)
    
    # Server params
    parser.add_argument("--port", type=int, default=5000)
    parser.add_argument("--host", type=str, default="0.0.0.0")
    
    # prm parameters
    parser.add_argument("--placeholder_token", type=str, default=None)
    parser.add_argument("--reward_tokens", type=str, nargs="*", default=None)
    
    args = parser.parse_args()
    
    # åˆå§‹åŒ–æ¨¡å‹
    reward_model = GenPRM(args)
    
    # åˆ›å»ºFastAPIåº”ç”¨
    app = FastAPI()
    
    @app.post("/get_gen_prm")
    async def get_reward(request: Request):
        data = await request.json()
        queries = data.get("query", [])
        rewards = reward_model.get_reward(queries)
        return JSONResponse({"rewards": rewards})

    uvicorn.run(app, host=args.host, port=args.port, log_level="info")
